[VARS]
task_name = long_term_forecast
is_training = 10
model_id= test
model = Autoformer

data = ETTh1
root_path = ./data/ETT/
data_path = ETTh1.csv
features = MS
#help=forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate)
target = OT
#help=target feature in S or MS task
freq = w
#help=freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h
checkpoints = ./checkpoints/
#help=location of model checkpoints)
              
# forecasting task
seq_len = 52
#help=input sequence length
label_len = 26
#help=start token length
pred_len = 52
#help=prediction sequence length
seasonal_patterns=Monthly
#help=subset for M4
inverse = False
#help=inverse output data, default=False

# inputation task
mask_rate = 0.25
#help=mask ratio

# anomaly detection task
anomaly_ratio = 0.25
#help=prior anomaly ratio (%%))

 # model define
expand=2
#help=expansion factor for Mamba
d_conv=4
#help=conv kernel size for Mamba)
top_k=5
#help=for TimesBlock
num_kernels=6
#help=for Inception
enc_in=6
#help=encoder input size
dec_in=6
#help=decoder input size
c_out=6
#help=output size
d_model=512
#help=dimension of model
n_heads=8
#help=num of heads
e_layers=2
#help=num of encoder layers
d_layers=1
#help=num of decoder layers
d_ff=2048
#help=dimension of fcn
moving_avg=25
#help=window size of moving average
factor=1
#help=attn factor
distil=True
#help=whether to use distilling in encoder, using this argument means not using distilling
dropout=0.1
#help=dropout
embed=timeF
#help=time features encoding, options:[timeF, fixed, learned]
activation=gelu
#help=activation)
channel_independence=1
#help=0: channel dependence 1: channel independence for FreTS model)
decomp_method=moving_avg
#help=method of series decompsition, only support moving_avg or dft_decomp)
use_norm=1
#help=whether to use normalize; True 1 False 0)
down_sampling_layers=0
#help=num of down sampling layers)
down_sampling_window=1
#help=down sampling window size)
down_sampling_method=None
#help=down sampling method, only support avg, max, conv)
seg_len=96
#help=the length of segmen-wise iteration of SegRNN)

# optimization
num_workers=10
#help=data loader num workers)
itr=1
#help=experiments times)
train_epochs=10
#help=train epochs)
batch_size=32
#help=batch size of train input data)
patience=3
#help=early stopping patience)
learning_rate=0.0001
#help=optimizer learning rate)
des=test
#help=exp description)
loss=MSE
#help=loss function)
lradj=type1
#help=adjust learning rate)
use_amp= False
#help=use automatic mixed precision training

# TimeXer
patch_len=16
#help=patch length
